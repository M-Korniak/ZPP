{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "from typing import Callable, List, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a class to store model arguments for easier parameter testing, and another one to store parameters describing input format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    device: torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    batch_size: int = 16\n",
    "    epochs: int = 20\n",
    "    learning_rate: float = 0.001\n",
    "    criterion: nn.Module = nn.L1Loss()\n",
    "    optimizer: Callable = optim.Adam  # Domyślnie używamy Adam\n",
    "\n",
    "    def get_optimizer(self, model):\n",
    "        return optim.Adam(model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "@dataclass\n",
    "class InputArgs:\n",
    "    transform: Callable = transforms.Compose([\n",
    "        transforms.ToTensor()  # Transform to [0, 1] range\n",
    "    ])\n",
    "    source_dir: str = os.getcwd() + \"/data/example\" # Directory where data is stored\n",
    "    test_size: float = 0.2  # Ratio for train-test split\n",
    "    crop_percentage: float = 0.2  # Percentage of the image to crop\n",
    "    depth: int = 1\n",
    "    use_grid: bool = False  # Whether to divide frames into grid or center-crop\n",
    "    use_random_crop = False\n",
    "    rotation_angle = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_sorted_image_paths(source_dir: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieve all .jpg file paths from the given directory and subdirectories,\n",
    "    sorted lexicographically.\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        for file in sorted(files):\n",
    "            if file.lower().endswith('.jpg'):\n",
    "                image_paths.append(os.path.join(root, file))\n",
    "    return image_paths\n",
    "\n",
    "def load_images(image_paths: List[str], depth: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Load images from the given paths, stack them into a tensor of the specified depth\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    for path in image_paths:\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        img_tensor = transforms.ToTensor()(img)\n",
    "        images.append(img_tensor)\n",
    "        if len(images) == depth:\n",
    "            yield torch.stack(images)\n",
    "            images = []\n",
    "    if images:\n",
    "        yield torch.stack(images)\n",
    "\n",
    "def crop_into_grid(image: torch.Tensor, crop_percentage: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Crop the image into a grid based on the given crop percentage and return a 2D tensor\n",
    "    of the cropped parts.\n",
    "    \"\"\"\n",
    "    _, h, w = image.shape\n",
    "    crop_size = int(min(h, w) * crop_percentage)\n",
    "    grid = []\n",
    "    for i in range(0, h - crop_size + 1, crop_size):\n",
    "        for j in range(0, w - crop_size + 1, crop_size):\n",
    "            cropped = image[:, i:i+crop_size, j:j+crop_size]\n",
    "            grid.append(cropped)\n",
    "    return torch.stack(grid)\n",
    "\n",
    "def crop_center(image: torch.Tensor, crop_percentage: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Crop the center of the image based on the given crop percentage.\n",
    "    \"\"\"\n",
    "    _, h, w = image.shape\n",
    "    crop_size = int(min(h, w) * crop_percentage)\n",
    "    top = (h - crop_size) // 2\n",
    "    left = (w - crop_size) // 2\n",
    "    return image[:, top:top+crop_size, left:left+crop_size]\n",
    "\n",
    "def crop_random(image: torch.Tensor, crop_percentage: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Crop a random region of the image based on the given crop percentage.\n",
    "    \"\"\"\n",
    "    _, h, w = image.shape\n",
    "    crop_size = int(min(h, w) * crop_percentage)\n",
    "\n",
    "    top = random.randint(0, h - crop_size)\n",
    "    left = random.randint(0, w - crop_size)\n",
    "\n",
    "    return image[:, top:top+crop_size, left:left+crop_size]\n",
    "\n",
    "\n",
    "def rotate_image(image: torch.Tensor, angle: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Rotates an image tensor by a specified angle.\n",
    "\n",
    "    :param image: A tensor representing the image in (C, H, W) format.\n",
    "    :param angle: The rotation angle in degrees (positive values rotate counterclockwise).\n",
    "    :return: The rotated image as a tensor.\n",
    "    \"\"\"\n",
    "    return TF.rotate(image, angle)\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths: List[str], transform: transforms.Compose = None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img_tensor = transforms.ToTensor()(img)\n",
    "        if self.transform:\n",
    "            img_tensor = self.transform(img_tensor)\n",
    "        return img_tensor\n",
    "\n",
    "def process_images(source_dir: str, crop_percentage: float, batch_size: int, test_size: float, use_grid: bool, use_random_crop: bool, rotation_angle: int):\n",
    "    \"\"\"\n",
    "    Process images from the source directory: load, crop, and split into train and test sets.\n",
    "    \"\"\"\n",
    "    image_paths = get_sorted_image_paths(source_dir)\n",
    "    transform_list = []\n",
    "\n",
    "    if use_grid:\n",
    "        transform_list.append(lambda x: crop_into_grid(x, crop_percentage))\n",
    "    elif use_random_crop:\n",
    "        transform_list.append(lambda x: crop_random(x, crop_percentage))\n",
    "    else:\n",
    "        transform_list.append(lambda x: crop_center(x, crop_percentage))\n",
    "\n",
    "    if rotation_angle != 0:\n",
    "        transform_list.append(lambda x: rotate_image(x, rotation_angle))\n",
    "\n",
    "    transform = transforms.Compose(transform_list)\n",
    "    dataset = ImageDataset(image_paths, transform=transform)\n",
    "\n",
    "    # Calculate the sizes for train and test datasets\n",
    "    total_size = len(dataset)\n",
    "    test_size = int(total_size * test_size)\n",
    "    train_size = total_size - test_size\n",
    "\n",
    "    # Split the dataset into train and test sets\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# TODO : now depth is always = 1; change it!\n",
    "# (now we only consider the neighbourhood in the given time, not looking at past frames)\n",
    "# TODO : add the possibility to keep the original or close to original sizes, but with lower resolution\n",
    "\n",
    "# Example usage:\n",
    "params = InputArgs\n",
    "model_params = ModelArgs\n",
    "train_loader, test_loader = process_images(params.source_dir, params.crop_percentage, model_params.batch_size, params.test_size, params.use_grid, params.use_random_crop, params.rotation_angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Optional\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, sizes: Optional[List[int]] = None, kernel_sizes: Optional[List[int]] = None,\n",
    "                 strides: Optional[List[int]] = None, paddings: Optional[List[int]] = None):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        # Smaller sizes for less memory usage\n",
    "        sizes = sizes or [32, 64, 128, 256]  # Reduced filter sizes\n",
    "        kernel_sizes = kernel_sizes or [3] * (len(sizes) - 1)\n",
    "        strides = strides or [1] * (len(sizes) - 1)\n",
    "        paddings = paddings or [1] * (len(sizes) - 1)\n",
    "\n",
    "        # Encoder layers with MaxPooling\n",
    "        self.encoder = []\n",
    "        in_channels = 3  # RGB input\n",
    "\n",
    "        for i in range(len(sizes) - 1):\n",
    "            self.encoder.append(\n",
    "                nn.Conv2d(in_channels, sizes[i], kernel_size=kernel_sizes[i], stride=strides[i], padding=paddings[i])\n",
    "            )\n",
    "            self.encoder.append(nn.BatchNorm2d(sizes[i]))\n",
    "            self.encoder.append(nn.ReLU())\n",
    "            self.encoder.append(nn.MaxPool2d(kernel_size=2, stride=2))  # Downsampling by 2\n",
    "            in_channels = sizes[i]\n",
    "\n",
    "        self.encoder = nn.Sequential(*self.encoder)\n",
    "\n",
    "        # Decoder layers with Upsampling\n",
    "        self.decoder = []\n",
    "        for i in range(len(sizes) - 2, -1, -1):\n",
    "            self.decoder.append(\n",
    "                nn.ConvTranspose2d(in_channels, sizes[i], kernel_size=kernel_sizes[i], stride=2, padding=paddings[i], output_padding=1)\n",
    "            )\n",
    "            self.decoder.append(nn.BatchNorm2d(sizes[i]))\n",
    "            self.decoder.append(nn.ReLU())\n",
    "            in_channels = sizes[i]\n",
    "\n",
    "        # Final layer to reconstruct the image\n",
    "        self.decoder.append(nn.ConvTranspose2d(in_channels, 3, kernel_size=kernel_sizes[-1], stride=1, padding=paddings[-1]))\n",
    "        self.decoder.append(nn.Sigmoid())  # Output in range [0,1]\n",
    "\n",
    "        self.decoder = nn.Sequential(*self.decoder)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "    def get_latent_space(self, x):\n",
    "        \"\"\"\n",
    "        Extract the latent space representation from the encoder.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            latent = self.encoder(x)\n",
    "        return latent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# With default architecture\n",
    "# model = Autoencoder()#.to(model_params.device)\n",
    "model = Autoencoder().to(ModelArgs().device)\n",
    "# model.to(model_params.device)\n",
    "model_params = ModelArgs()\n",
    "model.train()\n",
    "model_params.optimizer = model_params.get_optimizer(model)\n",
    "\n",
    "\n",
    "for epoch in range(model_params.epochs):\n",
    "    train_loss = 0\n",
    "\n",
    "    for images in train_loader:\n",
    "        images = images.to(model_params.device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = model_params.criterion(outputs, images)\n",
    "\n",
    "        model_params.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        model_params.optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{model_params.epochs}], Loss: {train_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing and visualisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    examples = next(iter(test_loader))\n",
    "    test_images = examples\n",
    "    test_images = test_images.to(model_params.device)\n",
    "    reconstructed = model(test_images)\n",
    "\n",
    "    test_images = test_images.clamp(0, 1)\n",
    "    reconstructed = reconstructed.clamp(0, 1)\n",
    "\n",
    "    # Visualisation of the reconstruction\n",
    "    n = 4\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    for i in range(n):\n",
    "        # Original cropped images\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(test_images[i].cpu().permute(1, 2, 0))\n",
    "        plt.title(\"Original\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Reconstructed images\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(reconstructed[i].cpu().permute(1, 2, 0))\n",
    "        plt.title(\"Reconstruction\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Getting the statistics and testing out different parameters "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
