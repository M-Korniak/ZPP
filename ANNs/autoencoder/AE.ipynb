{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "from typing import Callable, List, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a class to store model arguments for easier parameter testing, and another one to store parameters describing input format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    device: torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    batch_size: int = 256\n",
    "    epochs: int = 20\n",
    "    learning_rate: float = 0.001\n",
    "    criterion: nn.Module = nn.L1Loss()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InputArgs:\n",
    "    transform: Callable = transforms.Compose([\n",
    "        transforms.ToTensor()  # Transform to [0, 1] range\n",
    "    ])\n",
    "    source_dir: str = os.getcwd() + \"/data/example\" # Directory where data is stored\n",
    "    test_size: float = 0.2  # Ratio for train-test split\n",
    "    crop_percentage: float = 0.2  # Percentage of the image to crop\n",
    "    depth: int = 1\n",
    "    use_grid: bool = False  # Whether to divide frames into grid or center-crop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_sorted_image_paths(source_dir: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieve all .jpg file paths from the given directory and subdirectories,\n",
    "    sorted lexicographically.\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        for file in sorted(files):\n",
    "            if file.lower().endswith('.jpg'):\n",
    "                image_paths.append(os.path.join(root, file))\n",
    "    return image_paths\n",
    "\n",
    "def load_images(image_paths: List[str], depth: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Load images from the given paths, stack them into a tensor of the specified depth\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    for path in image_paths:\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        img_tensor = transforms.ToTensor()(img)\n",
    "        images.append(img_tensor)\n",
    "        if len(images) == depth:\n",
    "            yield torch.stack(images)\n",
    "            images = []\n",
    "    if images:\n",
    "        yield torch.stack(images)\n",
    "\n",
    "def crop_into_grid(image: torch.Tensor, crop_percentage: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Crop the image into a grid based on the given crop percentage and return a 2D tensor\n",
    "    of the cropped parts.\n",
    "    \"\"\"\n",
    "    _, h, w = image.shape\n",
    "    crop_size = int(min(h, w) * crop_percentage)\n",
    "    grid = []\n",
    "    for i in range(0, h - crop_size + 1, crop_size):\n",
    "        for j in range(0, w - crop_size + 1, crop_size):\n",
    "            cropped = image[:, i:i+crop_size, j:j+crop_size]\n",
    "            grid.append(cropped)\n",
    "    return torch.stack(grid)\n",
    "\n",
    "def crop_center(image: torch.Tensor, crop_percentage: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Crop the center of the image based on the given crop percentage.\n",
    "    \"\"\"\n",
    "    _, h, w = image.shape\n",
    "    crop_size = int(min(h, w) * crop_percentage)\n",
    "    top = (h - crop_size) // 2\n",
    "    left = (w - crop_size) // 2\n",
    "    return image[:, top:top+crop_size, left:left+crop_size]\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths: List[str], transform: transforms.Compose = None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img_tensor = transforms.ToTensor()(img)\n",
    "        if self.transform:\n",
    "            img_tensor = self.transform(img_tensor)\n",
    "        return img_tensor\n",
    "\n",
    "def process_images(source_dir: str, crop_percentage: float, batch_size: int, test_size: float, use_grid: bool):\n",
    "    \"\"\"\n",
    "    Process images from the source directory: load, crop, and split into train and test sets.\n",
    "    \"\"\"\n",
    "    image_paths = get_sorted_image_paths(source_dir)\n",
    "    if use_grid:\n",
    "        dataset = ImageDataset(image_paths, transform=lambda x: crop_into_grid(x, crop_percentage))\n",
    "    else:\n",
    "        dataset = ImageDataset(image_paths, transform=lambda x: crop_center(x, crop_percentage))\n",
    "    \n",
    "    # Calculate the sizes for train and test datasets\n",
    "    total_size = len(dataset)\n",
    "    test_size = int(total_size * test_size)\n",
    "    train_size = total_size - test_size\n",
    "\n",
    "    # Split the dataset into train and test sets\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# TODO : now depth is always = 1; change it!\n",
    "# (now we only consider the neighbourhood in the given time, not looking at past frames)\n",
    "# TODO : add the possibility to keep the original or close to original sizes, but with lower resolution\n",
    "\n",
    "# Example usage:\n",
    "params = InputArgs\n",
    "model_params = ModelArgs\n",
    "train_loader, test_loader = process_images(params.source_dir, params.crop_percentage, model_params.batch_size, params.test_size, params.use_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, sizes: Optional[List[int]] = None, kernel_sizes: Optional[List[int]] = None,\n",
    "                 strides: Optional[List[int]] = None, paddings: Optional[List[int]] = None):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        # Default values for sizes, kernel sizes, strides, and paddings if not provided\n",
    "        sizes = sizes or [64, 128, 256, 512, 1024, 2048, 4096]\n",
    "        kernel_sizes = kernel_sizes or [4] * (len(sizes) - 1)\n",
    "        strides = strides or [2] * (len(sizes) - 1)\n",
    "        paddings = paddings or [1] * (len(sizes) - 1)\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder = []\n",
    "        in_channels = 3  # Start with 3 input channels (for RGB images)\n",
    "        # But TODO here the past frames should contribute to more in_channels\n",
    "        for i in range(len(sizes) - 1):\n",
    "            self.encoder.append(\n",
    "                nn.Conv2d(in_channels, sizes[i], kernel_size=kernel_sizes[i], stride=strides[i], padding=paddings[i])\n",
    "            )\n",
    "            self.encoder.append(nn.BatchNorm2d(sizes[i]))\n",
    "            self.encoder.append(nn.ReLU())\n",
    "            in_channels = sizes[i]\n",
    "\n",
    "        self.encoder = nn.Sequential(*self.encoder)\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder = []\n",
    "        for i in range(len(sizes) - 2, -1, -1):\n",
    "            self.decoder.append(\n",
    "                nn.ConvTranspose2d(in_channels, sizes[i], kernel_size=kernel_sizes[i], stride=strides[i], padding=paddings[i])\n",
    "            )\n",
    "            self.decoder.append(nn.BatchNorm2d(sizes[i]))\n",
    "            self.decoder.append(nn.ReLU())\n",
    "            in_channels = sizes[i]\n",
    "\n",
    "        self.decoder.append(nn.ConvTranspose2d(in_channels, 3, kernel_size=kernel_sizes[-1], stride=strides[-1], padding=paddings[-1]))\n",
    "        self.decoder.append(nn.Sigmoid())  # Final output layer - outputs from the range [0,1]\n",
    "\n",
    "        self.decoder = nn.Sequential(*self.decoder)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    def get_latent_space(self, x):\n",
    "        \"\"\"\n",
    "        Extract the latent space representation from the encoder.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            latent = self.encoder(x)\n",
    "        return latent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With default architecture\n",
    "model = Autoencoder()#.to(model_params.device)\n",
    "model.to(model_params.device)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(model_params.epochs):\n",
    "    train_loss = 0\n",
    "    for images in train_loader:\n",
    "        images = images.to(model_params.device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = model_params.criterion(outputs, images)\n",
    "\n",
    "        model_params.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        model_params.optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{model_params.epochs}], Loss: {train_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing and visualisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    examples = next(iter(test_loader))\n",
    "    test_images, _ = examples\n",
    "    test_images = test_images.to(model_params.device)\n",
    "    reconstructed = model(test_images)\n",
    "\n",
    "    test_images = test_images.clamp(0, 1)\n",
    "    reconstructed = reconstructed.clamp(0, 1)\n",
    "\n",
    "    # Visualisation of the reconstruction\n",
    "    n = 4\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    for i in range(n):\n",
    "        # Original cropped images\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(test_images[i].cpu().permute(1, 2, 0))\n",
    "        plt.title(\"Original\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Reconstructed images\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(reconstructed[i].cpu().permute(1, 2, 0))\n",
    "        plt.title(\"Reconstruction\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Getting the statistics and testing out different parameters "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
